{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0864a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os, re, json, requests, time, argparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Callable, Tuple\n",
    "from langchain import hub\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4285970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000002C768FDADE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'structured-chat-agent', 'lc_hub_commit_hash': 'ea510f70a5872eb0f41a4e3b7bb004d5711dc127adee08329c664c6c8be5f13c'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], input_types={}, partial_variables={}, template='Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\n\\n{tools}\\n\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={}, template='{input}\\n\\n{agent_scratchpad}\\n (reminder to respond in a JSON blob no matter what)'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18154c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Tools\n",
    "# ------------------------------\n",
    "def tool_calc(query: str) -> str:\n",
    "    \"\"\"Safe arithmetic (+,-,*,/,**,(), integer/float).\"\"\"\n",
    "    allowed = re.compile(r\"^[\\d\\.\\+\\-\\*\\/\\(\\)\\s\\^eE]+$\")\n",
    "    q = query.replace(\"^\", \"**\")\n",
    "    if not allowed.match(q):\n",
    "        return \"Error: only arithmetic is allowed.\"\n",
    "    try:\n",
    "        # Very small sandbox: eval with empty globals/locals\n",
    "        result = eval(q, {\"__builtins__\": {}}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def tool_rag(query: str, api_url: str) -> str:\n",
    "    \"\"\"Call your existing RAG API: POST /ask -> {answer, sources[]}\"\"\"\n",
    "    try:\n",
    "        r = requests.post(api_url, json={\"question\": query}, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        ans = data.get(\"answer\", \"\")\n",
    "        srcs = data.get(\"sources\", [])\n",
    "        if srcs:\n",
    "            src_str = \"; \".join([f\"{s.get('doc','?')}#{s.get('section_id','?')}\" for s in srcs])\n",
    "            return f\"{ans}\\n[근거] {src_str}\"\n",
    "        return ans or \"(no answer)\"\n",
    "    except Exception as e:\n",
    "        return f\"RAG error: {e}\"\n",
    "\n",
    "@dataclass\n",
    "class ToolSpec:\n",
    "    name: str\n",
    "    description: str\n",
    "    func: Callable[[str], str]\n",
    "\n",
    "def build_tool_registry(rag_api: str | None) -> Dict[str, ToolSpec]:\n",
    "    reg = {\n",
    "        \"calc\": ToolSpec(\n",
    "            name=\"calc\",\n",
    "            description=\"산술 계산기 (+,-,*,/,^, 괄호 지원). 입력에는 계산식만 넣어.\",\n",
    "            func=tool_calc,\n",
    "        )\n",
    "    }\n",
    "    if rag_api:\n",
    "        reg[\"rag\"] = ToolSpec(\n",
    "            name=\"rag\",\n",
    "            description=\"사내 문서 기반 질문에 답하기. 입력: 자연어 질문.\",\n",
    "            func=lambda q: tool_rag(q, rag_api),\n",
    "        )\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb51744",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"\\\n",
    "너는 실용적인 도우미 Agent야. 네가 직접 모든 걸 알 필요는 없어.\n",
    "다음 '도구' 목록 중에서 필요한 도구를 골라 단계적으로 문제를 해결해.\n",
    "반드시 다음 형식을 따를 것:\n",
    "\n",
    "Thought: (현재 문제를 어떻게 풀지 생각)\n",
    "Action: <도구 이름 중 하나>\n",
    "Action Input: <도구에 전달할 입력 텍스트>\n",
    "Observation: <도구 실행 결과를 여기에 적음>\n",
    "\n",
    "... (필요하면 여러 번 반복) ...\n",
    "\n",
    "마지막에는 다음 형식으로 답변을 종료해:\n",
    "Final Answer: <사용자에게 줄 최종 답변>\n",
    "\n",
    "주의:\n",
    "- 모르면 도구(rag)를 사용하여 확인하거나, 모른다고 말해.\n",
    "- 환각(근거 없는 내용) 금지. 근거가 없으면 '모르겠다'고 해.\n",
    "- 답변은 한국어로.\n",
    "\"\"\"\n",
    "\n",
    "TOOL_FMT_HEADER = \"사용 가능한 도구 목록:\\n\"\n",
    "TOOL_FMT_ITEM = \"- {name}: {desc}\\n\"\n",
    "\n",
    "USER_WRAP = \"사용자 질문: {q}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a61f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_RE = re.compile(r\"Action:\\s*(\\w+)\\s*\", re.IGNORECASE)\n",
    "ACTION_INPUT_RE = re.compile(r\"Action Input:\\s*(.*)\", re.IGNORECASE)\n",
    "\n",
    "def run_agent(tools: Dict[str, ToolSpec], question: str, max_steps=4) -> str:\n",
    "    tool_desc = TOOL_FMT_HEADER + \"\".join(TOOL_FMT_ITEM.format(name=k, desc=v.description) for k, v in tools.items())\n",
    "    scratch = f\"{SYS_PROMPT}\\n{tool_desc}\\n{USER_WRAP.format(q=question)}\\n\"\n",
    "    print(scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.add_argument(\"--rag_api\", type=str, default=None, help=\"e.g., http://localhost:8000/ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d04bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.add_argument(\"--max_steps\", type=int, default=4)\n",
    "args = ap.parse_args([])  # set to [] for notebook; remove for real CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10874a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.rag_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13360ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = build_tool_registry(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e84a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(tools, \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa127477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_use import MCPAgent, MCPClient\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mcpServers\": {\n",
    "        \"name\": \"demo\",\n",
    "        \"transport\": \"stdio\",\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"src/mcp_test_server.py\", \"stdio\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03909ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCPClient from config file\n",
    "client = MCPClient.from_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8ed647",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAI(\n",
    "    base_url = os.getenv(\"ollama_url\") + \"v1\",\n",
    "    api_key='ollama', # required, but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f164a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played in **Boston**! Specifically, it was held at Fenway Park, though it was mostly played in empty stadiums due to the pandemic.\n"
     ]
    }
   ],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "  model=os.getenv(\"ollama_model\"),\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4686a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 11:26:39,339 - mcp_use.telemetry.telemetry - INFO - Anonymized telemetry enabled. Set MCP_USE_ANONYMIZED_TELEMETRY=false to disable.\n"
     ]
    }
   ],
   "source": [
    "agent = MCPAgent(\n",
    "    llm={\"type\": \"openai\", \"client\": llm_client, \"model\": os.getenv(\"ollama_model\")},\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6b1c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object MCPAgent.run at 0x0000023AA5B71240>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = MCPAgent.run(self=agent, query=\"2랑 3 더해봐.\", max_steps=5)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8bcfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-12 11:26:52,846 - mcp_use - INFO - 🚀 Initializing MCP agent and connecting to services...\n",
      "2025-09-12 11:26:52,847 - mcp_use - ERROR - ❌ Error running query: 'OpenAI' object has no attribute 'get_all_active_sessions'\n",
      "2025-09-12 11:26:52,847 - mcp_use - ERROR - ❌ Error during agent execution: 'OpenAI' object has no attribute 'get_all_active_sessions'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'get_all_active_sessions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:453\u001b[39m, in \u001b[36mMCPAgent.stream\u001b[39m\u001b[34m(self, query, max_steps, manage_connector, external_history, track_execution, output_schema)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manage_connector \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._initialized:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initialize()\n\u001b[32m    454\u001b[39m     initialized_here = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:178\u001b[39m, in \u001b[36mMCPAgent.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client:\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# First try to get existing sessions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28mself\u001b[39m._sessions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_active_sessions\u001b[49m()\n\u001b[32m    179\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔌 Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._sessions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m existing sessions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'get_all_active_sessions'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:837\u001b[39m, in \u001b[36mMCPAgent.run\u001b[39m\u001b[34m(self, query, max_steps, manage_connector, external_history, output_schema)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     result, steps_taken = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_and_return(generator)\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:377\u001b[39m, in \u001b[36mMCPAgent._consume_and_return\u001b[39m\u001b[34m(self, generator)\u001b[39m\n\u001b[32m    376\u001b[39m steps_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# If it's a string, it's the final result (regular output)\u001b[39;00m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:734\u001b[39m, in \u001b[36mMCPAgent.stream\u001b[39m\u001b[34m(self, query, max_steps, manage_connector, external_history, track_execution, output_schema)\u001b[39m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client:\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     server_count = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_active_sessions\u001b[49m())\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connectors:\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'get_all_active_sessions'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\FinalProjectAIWorkspace\\.venv\\Lib\\site-packages\\mcp_use\\agents\\mcpagent.py:851\u001b[39m, in \u001b[36mMCPAgent.run\u001b[39m\u001b[34m(self, query, max_steps, manage_connector, external_history, output_schema)\u001b[39m\n\u001b[32m    843\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28mself\u001b[39m.telemetry.track_agent_execution(\n\u001b[32m    846\u001b[39m         execution_method=\u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    847\u001b[39m         query=query,\n\u001b[32m    848\u001b[39m         success=success,\n\u001b[32m    849\u001b[39m         model_provider=\u001b[38;5;28mself\u001b[39m._model_provider,\n\u001b[32m    850\u001b[39m         model_name=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m         server_count=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_active_sessions\u001b[49m()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.connectors),\n\u001b[32m    852\u001b[39m         server_identifiers=[connector.public_identifier \u001b[38;5;28;01mfor\u001b[39;00m connector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connectors],\n\u001b[32m    853\u001b[39m         total_tools_available=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._tools) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tools \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m    854\u001b[39m         tools_available_names=[tool.name \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tools],\n\u001b[32m    855\u001b[39m         max_steps_configured=\u001b[38;5;28mself\u001b[39m.max_steps,\n\u001b[32m    856\u001b[39m         memory_enabled=\u001b[38;5;28mself\u001b[39m.memory_enabled,\n\u001b[32m    857\u001b[39m         use_server_manager=\u001b[38;5;28mself\u001b[39m.use_server_manager,\n\u001b[32m    858\u001b[39m         max_steps_used=max_steps,\n\u001b[32m    859\u001b[39m         manage_connector=manage_connector,\n\u001b[32m    860\u001b[39m         external_history_used=external_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    861\u001b[39m         steps_taken=steps_taken,\n\u001b[32m    862\u001b[39m         tools_used_count=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.tools_used_names),\n\u001b[32m    863\u001b[39m         tools_used_names=\u001b[38;5;28mself\u001b[39m.tools_used_names,\n\u001b[32m    864\u001b[39m         response=\u001b[38;5;28mstr\u001b[39m(result),\n\u001b[32m    865\u001b[39m         execution_time_ms=\u001b[38;5;28mint\u001b[39m((time.time() - start_time) * \u001b[32m1000\u001b[39m),\n\u001b[32m    866\u001b[39m         error_type=error,\n\u001b[32m    867\u001b[39m         conversation_history_length=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._conversation_history),\n\u001b[32m    868\u001b[39m     )\n\u001b[32m    869\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'get_all_active_sessions'"
     ]
    }
   ],
   "source": [
    "response.send(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinalProjectAIWorkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
